{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# JSONからParquetに変換"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "割り当てられた12桁のAccountIDを設定してください"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accountid = 'xxxxxxxxxxxx'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from awsglue.transforms import *\n",
    "from awsglue.utils import getResolvedOptions\n",
    "from pyspark.context import SparkContext\n",
    "from awsglue.context import GlueContext\n",
    "from awsglue.job import Job\n",
    "from pyspark.sql import SQLContext\n",
    "from pyspark.sql.functions import year, month, date_format"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## initialize\n",
    "sc = SparkContext.getOrCreate()\n",
    "glueContext = GlueContext(sc)\n",
    "spark = glueContext.spark_session\n",
    "\n",
    "job = Job(glueContext)\n",
    "job.init('sh10sales_parquet')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DataSource"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datasource0 = glueContext.create_dynamic_frame.from_catalog(\n",
    "    database = \"workshop\", \n",
    "    table_name = \"json_sales\",\n",
    "    transformation_ctx = \"datasource0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Count: \", datasource0.count() )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datasource0.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convert \n",
    "Convert to standard Spark DataFrame to do trasformation to be continued"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = datasource0.toDF()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For large data sets, try to cache the data will accelerate later execution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Repatition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "yearAddedDf = df.withColumn(\"year\", year(df.time_id))\n",
    "monthAddedDf = yearAddedDf.withColumn(\"month\", month(yearAddedDf.time_id))\n",
    "yyyymmAddedDf = monthAddedDf.withColumn(\"yyyymm\", date_format(monthAddedDf.time_id, 'yyyyMM'))\n",
    "\n",
    "repartitionedDf = yyyymmAddedDf.repartition(\"yyyymm\")\n",
    "droppedDf = repartitionedDf.drop(\"yyyymm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "castedDf = droppedDf.withColumn(\"prod_id\", droppedDf.prod_id.cast(\"decimal(38,0)\")) \\\n",
    "    .withColumn(\"cust_id\", droppedDf.cust_id.cast(\"decimal(38,0)\")) \\\n",
    "    .withColumn(\"time_id\", droppedDf.time_id.cast(\"timestamp\")) \\\n",
    "    .withColumn(\"channel_id\", droppedDf.channel_id.cast(\"decimal(38,0)\")) \\\n",
    "    .withColumn(\"promo_id\", droppedDf.promo_id.cast(\"decimal(38,0)\")) \\\n",
    "    .withColumn(\"quantity_sold\", droppedDf.quantity_sold.cast(\"decimal(38,2)\")) \\\n",
    "    .withColumn(\"seller\", droppedDf.seller.cast(\"int\")) \\\n",
    "    .withColumn(\"fulfillment_center\", droppedDf.fulfillment_center.cast(\"int\")) \\\n",
    "    .withColumn(\"courier_org\", droppedDf.courier_org.cast(\"int\")) \\\n",
    "    .withColumn(\"tax_country\", droppedDf.tax_country.cast(\"varchar(3)\")) \\\n",
    "    .withColumn(\"tax_region\", droppedDf.tax_region.cast(\"varchar(3)\")) \\\n",
    "    .withColumn(\"amount_sold\", droppedDf.amount_sold.cast(\"decimal(38,2)\")) \\\n",
    "    .withColumn(\"year\", droppedDf.year.cast(\"int\")) \\\n",
    "    .withColumn(\"month\", droppedDf.month.cast(\"int\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Write in S3\n",
    "S3にYear/monthのパーティショニングしたデータを出力"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "castedDf.write.partitionBy(  [\"year\", \"month\"]).mode(\"overwrite\").parquet(\n",
    "    \"s3://bigdata-handson-{accountid}/data/parquet/sh10/sales\",compression='snappy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "job.commit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Sparkmagic (PySpark)",
   "language": "",
   "name": "pysparkkernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "python",
    "version": 2
   },
   "mimetype": "text/x-python",
   "name": "pyspark",
   "pygments_lexer": "python2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
